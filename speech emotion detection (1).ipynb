{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT THE LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM,BatchNormalization , GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import tensorflow as tf \n",
    "print (\"Done\")\n",
    "Done\n",
    "!apt-get update\n",
    "!apt-get install -y libsndfile1\n",
    "Get:1 http://packages.cloud.google.com/apt gcsfuse-focal InRelease [5002 B]\n",
    "Get:2 https://packages.cloud.google.com/apt cloud-sdk InRelease [6361 B]\n",
    "Get:3 https://packages.cloud.google.com/apt google-fast-socket InRelease [5015 B]\n",
    "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
    "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
    "Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
    "Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
    "Get:8 http://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [2356 B]\n",
    "Get:9 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [474 kB]\n",
    "Get:10 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [447 B]\n",
    "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
    "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1079 kB]\n",
    "Get:13 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
    "Get:14 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1066 kB]\n",
    "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3336 kB]\n",
    "Get:16 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2856 kB]\n",
    "Get:17 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2479 kB]\n",
    "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2636 kB]\n",
    "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [39.5 kB]\n",
    "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1366 kB]\n",
    "Get:21 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
    "Get:22 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
    "Fetched 15.8 MB in 2s (7527 kB/s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "libsndfile1 is already the newest version (1.0.28-7ubuntu0.1).\n",
    "libsndfile1 set to manually installed.\n",
    "0 upgraded, 0 newly installed, 0 to remove and 193 not upgraded.\n",
    "Importing Data\n",
    "                                          Ravdess Dataframe\n",
    "Here is the filename identifiers as per the official RAVDESS website:\n",
    "\n",
    "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "Vocal channel (01 = speech, 02 = song).\n",
    "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "So, here's an example of an audio filename. 02-01-06-01-02-01-12.mp4 This means the meta data for the audio file is:\n",
    "\n",
    "Video-only (02)\n",
    "Speech (01)\n",
    "Fearful (06)\n",
    "Normal intensity (01)\n",
    "Statement \"dogs\" (02)\n",
    "1st Repetition (01)\n",
    "12th Actor (12) - Female (as the actor ID number is even)\n",
    "#preparing data set\n",
    "\n",
    "ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n",
    "ravdess_directory_list = os.listdir(ravdess)\n",
    "print(ravdess_directory_list)\n",
    "['Actor_02', 'Actor_17', 'Actor_05', 'Actor_16', 'Actor_21', 'Actor_01', 'Actor_11', 'Actor_20', 'Actor_08', 'Actor_15', 'Actor_06', 'Actor_12', 'Actor_23', 'Actor_24', 'Actor_22', 'Actor_04', 'Actor_19', 'Actor_10', 'Actor_09', 'Actor_14', 'Actor_03', 'Actor_13', 'Actor_18', 'Actor_07']\n",
    "Crema = \"/kaggle/input/cremad/AudioWAV/\"\n",
    "Tess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
    "Savee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\n",
    "preprocessing\n",
    "Ravdees\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for i in ravdess_directory_list:\n",
    "    # as their are 24 different actors in our previous directory we need to extract files for each actor.\n",
    "    actor = os.listdir(ravdess + i)\n",
    "    for f in actor:\n",
    "        part = f.split('.')[0].split('-')\n",
    "    # third part in each file represents the emotion associated to that file.\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(ravdess + i + '/' + f)\n",
    "    \n",
    "print(actor[0])\n",
    "print(part[0])\n",
    "print(file_path[0])\n",
    "print(int(part[2]))\n",
    "print(f)\n",
    "03-01-06-02-01-01-07.wav\n",
    "03\n",
    "/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_02/03-01-08-01-01-01-02.wav\n",
    "5\n",
    "03-01-05-02-01-02-07.wav\n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "# changing integers to actual emotions.\n",
    "ravdess_df.Emotions.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust',\n",
    "                             8:'surprise'},\n",
    "                            inplace=True)\n",
    "print(ravdess_df.head())\n",
    "print(\"______________________________________________\")\n",
    "print(ravdess_df.tail())\n",
    "print(\"_______________________________________________\")\n",
    "print(ravdess_df.Emotions.value_counts())\n",
    "   Emotions                                               Path\n",
    "0  surprise  /kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "1   neutral  /kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "2   disgust  /kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "3   disgust  /kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "4   neutral  /kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "______________________________________________\n",
    "     Emotions                                               Path\n",
    "1435     fear  /kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "1436    angry  /kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "1437      sad  /kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "1438  disgust  /kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "1439    angry  /kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "_______________________________________________\n",
    "neutral     288\n",
    "surprise    192\n",
    "disgust     192\n",
    "fear        192\n",
    "sad         192\n",
    "happy       192\n",
    "angry       192\n",
    "Name: Emotions, dtype: int64\n",
    "Crema DataFrame\n",
    "\n",
    "CREMA-D is a data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified). Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).\n",
    "\n",
    "crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in crema_directory_list:\n",
    "    # storing file paths\n",
    "    file_path.append(Crema + file)\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Crema_df.head()\n",
    "print(Crema_df.Emotions.value_counts())\n",
    "disgust    1271\n",
    "happy      1271\n",
    "sad        1271\n",
    "fear       1271\n",
    "angry      1271\n",
    "neutral    1087\n",
    "Name: Emotions, dtype: int64\n",
    "TESS dataset\n",
    "\n",
    "There are a set of 200 target words were spoken in the carrier phrase \"Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 data points (audio files) in total.\n",
    "\n",
    "The dataset is organised such that each of the two female actor and their emotions are contain within its own folder. And within that, all 200 target words audio file can be found. The format of the audio file is a WAV format\n",
    "\n",
    "tess_directory_list = os.listdir(Tess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "        if part=='ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Tess_df.head()\n",
    "print(Tess_df.Emotions.value_counts())\n",
    "fear        400\n",
    "angry       400\n",
    "disgust     400\n",
    "neutral     400\n",
    "sad         400\n",
    "surprise    400\n",
    "happy       400\n",
    "Name: Emotions, dtype: int64\n",
    "SAVEE Dataset\n",
    "\n",
    "Context The SAVEE database was recorded from four native English male speakers (identified as DC, JE, JK, KL), postgraduate students and researchers at the University of Surrey aged from 27 to 31 years. Emotion has been described psychologically in discrete categories: anger, disgust, fear, happiness, sadness and surprise. This is supported by the cross-cultural studies of Ekman [6] and studies of automatic emotion recognition tended to focus on recognizing these [12]. We added neutral to provide recordings of 7 emotion categories. The text material consisted of 15 TIMIT sentences per emotion: 3 common, 2 emotion-specific and 10 generic sentences that were different for each emotion and phonetically-balanced. The 3 common and 2 × 6 = 12 emotion-specific sentences were recorded as neutral to give 30 neutral sentences.\n",
    "\n",
    "Content This results in a total of 120 utterances per speaker, for example:\n",
    "\n",
    "Common: She had your dark suit in greasy wash water all year. Anger: Who authorized the unlimited expense account? Disgust: Please take this dirty table cloth to the cleaners for me. Fear: Call an ambulance for medical assistance. Happiness: Those musicians harmonize marvelously. Sadness: The prospect of cutting back spending is an unpleasant one for any governor. Surprise: The carpet cleaners shampooed our oriental rug. Neutral: The best way to learn is to solve extra problems.\n",
    "\n",
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]\n",
    "    if ele=='a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele=='d':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele=='f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele=='h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele=='n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele=='sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Savee_df.head()\n",
    "print(Savee_df.Emotions.value_counts())\n",
    "neutral     120\n",
    "happy        60\n",
    "fear         60\n",
    "disgust      60\n",
    "angry        60\n",
    "surprise     60\n",
    "sad          60\n",
    "Name: Emotions, dtype: int64\n",
    "Integration\n",
    "\n",
    "# creating Dataframe using all the 4 dataframes we created so far.\n",
    "data_path = pd.concat([ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\n",
    "data_path.to_csv(\"data_path.csv\",index=False)\n",
    "data_path.head()\n",
    "Emotions\tPath\n",
    "0\tsurprise\t/kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "1\tneutral\t/kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "2\tdisgust\t/kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "3\tdisgust\t/kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "4\tneutral\t/kaggle/input/ravdess-emotional-speech-audio/a...\n",
    "print(data_path.Emotions.value_counts())\n",
    "disgust     1923\n",
    "fear        1923\n",
    "sad         1923\n",
    "happy       1923\n",
    "angry       1923\n",
    "neutral     1895\n",
    "surprise     652\n",
    "Name: Emotions, dtype: int64\n",
    "Data Visualisation and Exploration\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(data_path.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()\n",
    "\n",
    "data,sr = librosa.load(file_path[0])\n",
    "sr\n",
    "22050\n",
    "ipd.Audio(data,rate=sr)\n",
    "# CREATE LOG MEL SPECTROGRAM\n",
    "plt.figure(figsize=(10, 5))\n",
    "spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \n",
    "log_spectrogram = librosa.power_to_db(spectrogram)\n",
    "librosa.display.specshow(log_spectrogram, y_axis='mel', sr=sr, x_axis='time');\n",
    "plt.title('Mel Spectrogram ')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "<matplotlib.colorbar.Colorbar at 0x7d69a9104550>\n",
    "\n",
    "mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=30)\n",
    "\n",
    "\n",
    "# MFCC\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.specshow(mfcc, x_axis='time')\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()\n",
    "\n",
    "ipd.Audio(data,rate=sr)\n",
    "\n",
    "Data augmentation\n",
    "# NOISE\n",
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "# STRETCH\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "# SHIFT\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    return np.roll(data, shift_range)\n",
    "# PITCH\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
    "# NORMAL AUDIO\n",
    "\n",
    "\n",
    "import librosa.display\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=data, sr=sr)\n",
    "ipd.Audio(data,rate=sr)\n",
    "\n",
    "# AUDIO WITH NOISE\n",
    "x = noise(data)\n",
    "plt.figure(figsize=(12,5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)\n",
    "\n",
    "# STRETCHED AUDIO\n",
    "x = stretch(data)\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)\n",
    "\n",
    "# SHIFTED AUDIO\n",
    "x = shift(data)\n",
    "plt.figure(figsize=(12,5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)\n",
    "\n",
    "# AUDIO WITH PITCH\n",
    "x = pitch(data, sr)\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)\n",
    "\n",
    "Feature extraction\n",
    "def zcr(data,frame_length,hop_length):\n",
    "    zcr=librosa.feature.zero_crossing_rate(data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "def rmse(data,frame_length=2048,hop_length=512):\n",
    "    rmse=librosa.feature.rms(data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(rmse)\n",
    "def mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n",
    "    mfcc=librosa.feature.mfcc(data,sr=sr)\n",
    "    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n",
    "\n",
    "def extract_features(data,sr=22050,frame_length=2048,hop_length=512):\n",
    "    result=np.array([])\n",
    "    \n",
    "    result=np.hstack((result,\n",
    "                      zcr(data,frame_length,hop_length),\n",
    "                      rmse(data,frame_length,hop_length),\n",
    "                      mfcc(data,sr,frame_length,hop_length)\n",
    "                     ))\n",
    "    return result\n",
    "\n",
    "def get_features(path,duration=2.5, offset=0.6):\n",
    "    data,sr=librosa.load(path,duration=duration,offset=offset)\n",
    "    aud=extract_features(data)\n",
    "    audio=np.array(aud)\n",
    "    \n",
    "    noised_audio=noise(data)\n",
    "    aud2=extract_features(noised_audio)\n",
    "    audio=np.vstack((audio,aud2))\n",
    "    \n",
    "    pitched_audio=pitch(data,sr)\n",
    "    aud3=extract_features(pitched_audio)\n",
    "    audio=np.vstack((audio,aud3))\n",
    "    \n",
    "    pitched_audio1=pitch(data,sr)\n",
    "    pitched_noised_audio=noise(pitched_audio1)\n",
    "    aud4=extract_features(pitched_noised_audio)\n",
    "    audio=np.vstack((audio,aud4))\n",
    "    \n",
    "    return audio\n",
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())\n",
    "Number of processors:  2\n",
    "Noraml way to get features\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "start = timeit.default_timer()\n",
    "X,Y=[],[]\n",
    "for path,emotion,index in tqdm (zip(data_path.Path,data_path.Emotions,range(data_path.Path.shape[0]))):\n",
    "    features=get_features(path)\n",
    "    if index%500==0:\n",
    "        print(f'{index} audio has been processed')\n",
    "    for i in features:\n",
    "        X.append(i)\n",
    "        Y.append(emotion)\n",
    "print('Done')\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)         \n",
    "1it [00:00,  2.07it/s]\n",
    "0 audio has been processed\n",
    "501it [04:45,  1.77it/s]\n",
    "500 audio has been processed\n",
    "1001it [09:30,  1.80it/s]\n",
    "1000 audio has been processed\n",
    "1501it [14:06,  2.28it/s]\n",
    "1500 audio has been processed\n",
    "2001it [17:49,  2.32it/s]\n",
    "2000 audio has been processed\n",
    "2501it [21:35,  2.12it/s]\n",
    "2500 audio has been processed\n",
    "3001it [25:19,  2.26it/s]\n",
    "3000 audio has been processed\n",
    "3501it [29:05,  2.19it/s]\n",
    "3500 audio has been processed\n",
    "4001it [32:55,  2.25it/s]\n",
    "4000 audio has been processed\n",
    "4501it [36:41,  2.42it/s]\n",
    "4500 audio has been processed\n",
    "5001it [40:28,  2.25it/s]\n",
    "5000 audio has been processed\n",
    "5501it [44:17,  2.23it/s]\n",
    "5500 audio has been processed\n",
    "6001it [48:03,  2.06it/s]\n",
    "6000 audio has been processed\n",
    "6501it [51:51,  2.10it/s]\n",
    "6500 audio has been processed\n",
    "7001it [55:38,  2.24it/s]\n",
    "7000 audio has been processed\n",
    "7501it [59:24,  2.23it/s]\n",
    "7500 audio has been processed\n",
    "8001it [1:03:11,  2.32it/s]\n",
    "8000 audio has been processed\n",
    "8501it [1:06:56,  2.14it/s]\n",
    "8500 audio has been processed\n",
    "9001it [1:10:30,  2.98it/s]\n",
    "9000 audio has been processed\n",
    "9501it [1:13:16,  2.30it/s]\n",
    "9500 audio has been processed\n",
    "10001it [1:16:51,  2.25it/s]\n",
    "10000 audio has been processed\n",
    "10501it [1:20:49,  2.40it/s]\n",
    "10500 audio has been processed\n",
    "11001it [1:24:20,  2.25it/s]\n",
    "11000 audio has been processed\n",
    "11501it [1:27:52,  2.70it/s]\n",
    "11500 audio has been processed\n",
    "12001it [1:31:50,  1.99it/s]\n",
    "12000 audio has been processed\n",
    "12162it [1:33:14,  2.17it/s]\n",
    "Done\n",
    "Time:  5594.533975138\n",
    "Faster way to get features\n",
    "Parallel way\n",
    "\n",
    "Dont be afraid from red lines that Normal\n",
    "\n",
    "This code is an example of how to use the joblib library to process multiple audio files in parallel using the process_feature function. The code also uses the timeit library to measure the time taken to process the audio files.\n",
    "\n",
    "Here's a breakdown of what the code does:\n",
    "\n",
    "The from joblib import Parallel, delayed statement imports the Parallel and delayed functions from the joblib library. The start = timeit.default_timer() statement starts a timer to measure the time taken to process the audio files. The process_feature function processes a single audio file by extracting its features using the get_feat function and appending the corresponding X and Y values to the X and Y lists. The paths and emotions variables extract the paths and emotions from the data_path DataFrame. The Parallel function runs the process_feature function in parallel for each audio file using the delayed function to wrap the process_feature function. The results variable contains the X and Y values for each audio file. The X and Y lists are populated with the X and Y values from each audio file using the extend method. The stop = timeit.default_timer() statement stops the timer. The print('Time: ', stop - start) statement prints the time taken to process the audio files. Overall, this code demonstrates how to use the joblib library to process multiple audio files in parallel, which can significantly reduce the processing time for large datasets.This code is an example of how to use the joblib library to process multiple audio files in parallel using the process_feature function. The code also uses the timeit library to measure the time taken to process the audio files.\n",
    "\n",
    "Here's a breakdown of what the code does:\n",
    "\n",
    "The from joblib import Parallel, delayed statement imports the Parallel and delayed functions from the joblib library. The start = timeit.default_timer() statement starts a timer to measure the time taken to process the audio files. The process_feature function processes a single audio file by extracting its features using the get_feat function and appending the corresponding X and Y values to the X and Y lists. The paths and emotions variables extract the paths and emotions from the data_path DataFrame. The Parallel function runs the process_feature function in parallel for each audio file using the delayed function to wrap the process_feature function. The results variable contains the X and Y values for each audio file. The X and Y lists are populated with the X and Y values from each audio file using the extend method. The stop = timeit.default_timer() statement stops the timer. The print('Time: ', stop - start) statement prints the time taken to process the audio files. Overall, this code demonstrates how to use the joblib library to process multiple audio files in parallel, which can significantly reduce the processing time for large datasets.\n",
    "\n",
    "The .extend() method increases the length of the list by the number of elements that are provided to the method, so if you want to add multiple elements to the list, you can use this method.\n",
    "\"\"\"from joblib import Parallel, delayed\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "# Define a function to get features for a single audio file\n",
    "def process_feature(path, emotion):\n",
    "    features = get_features(path)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for ele in features:\n",
    "        X.append(ele)\n",
    "        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
    "        Y.append(emotion)\n",
    "    return X, Y\n",
    "\n",
    "paths = data_path.Path\n",
    "emotions = data_path.Emotions\n",
    "\n",
    "# Run the loop in parallel\n",
    "results = Parallel(n_jobs=-1)(delayed(process_feature)(path, emotion) for (path, emotion) in zip(paths, emotions))\n",
    "\n",
    "# Collect the results\n",
    "X = []\n",
    "Y = []\n",
    "for result in results:\n",
    "    x, y = result\n",
    "    X.extend(x)\n",
    "    Y.extend(y)\n",
    "\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)    \"\"\"\n",
    "\"from joblib import Parallel, delayed\\nimport timeit\\nstart = timeit.default_timer()\\n# Define a function to get features for a single audio file\\ndef process_feature(path, emotion):\\n    features = get_features(path)\\n    X = []\\n    Y = []\\n    for ele in features:\\n        X.append(ele)\\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\\n        Y.append(emotion)\\n    return X, Y\\n\\npaths = data_path.Path\\nemotions = data_path.Emotions\\n\\n# Run the loop in parallel\\nresults = Parallel(n_jobs=-1)(delayed(process_feature)(path, emotion) for (path, emotion) in zip(paths, emotions))\\n\\n# Collect the results\\nX = []\\nY = []\\nfor result in results:\\n    x, y = result\\n    X.extend(x)\\n    Y.extend(y)\\n\\n\\nstop = timeit.default_timer()\\n\\nprint('Time: ', stop - start)    \"\n",
    "len(X), len(Y), data_path.Path.shape\n",
    "(48648, 48648, (12162,))\n",
    "Saving features\n",
    "Emotions = pd.DataFrame(X)\n",
    "Emotions['Emotions'] = Y\n",
    "Emotions.to_csv('emotion.csv', index=False)\n",
    "Emotions.head()\n",
    "0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t...\t2367\t2368\t2369\t2370\t2371\t2372\t2373\t2374\t2375\tEmotions\n",
    "0\t0.331543\t0.471680\t0.564941\t0.452148\t0.374512\t0.296875\t0.265137\t0.256348\t0.252441\t0.251953\t...\t-1.517647\t-1.716411\t-1.599245\t-1.234544\t-0.693115\t-0.038821\t0.675410\t1.405862\t2.112551\tsurprise\n",
    "1\t0.238770\t0.361816\t0.478516\t0.473633\t0.485352\t0.476074\t0.472656\t0.468262\t0.472656\t0.486816\t...\t2.192987\t1.378691\t1.542983\t1.766679\t-1.586487\t-0.501919\t-3.159530\t-5.015890\t-0.942531\tsurprise\n",
    "2\t0.299805\t0.419922\t0.525879\t0.459473\t0.378418\t0.326172\t0.282227\t0.260254\t0.258789\t0.253906\t...\t0.518931\t0.839123\t1.308965\t1.855116\t2.404975\t2.896071\t3.282977\t3.541091\t3.666846\tsurprise\n",
    "3\t0.252930\t0.382812\t0.497559\t0.497070\t0.487793\t0.472656\t0.482422\t0.484375\t0.497070\t0.510254\t...\t-2.445378\t-2.712779\t-1.120462\t-3.266942\t7.691891\t7.443986\t-2.031003\t-2.095720\t-1.418903\tsurprise\n",
    "4\t0.400879\t0.591309\t0.783203\t0.777832\t0.771973\t0.777832\t0.771973\t0.775391\t0.758789\t0.580078\t...\t2.585884\t2.746578\t2.223155\t0.986784\t-0.730886\t-2.531058\t-4.002848\t-4.849192\t-4.929412\tneutral\n",
    "5 rows × 2377 columns\n",
    "\n",
    "Emotions = pd.read_csv('./emotion.csv')\n",
    "Emotions.head()\n",
    "0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t...\t2367\t2368\t2369\t2370\t2371\t2372\t2373\t2374\t2375\tEmotions\n",
    "0\t0.331543\t0.471680\t0.564941\t0.452148\t0.374512\t0.296875\t0.265137\t0.256348\t0.252441\t0.251953\t...\t-1.517647\t-1.716411\t-1.599245\t-1.234544\t-0.693115\t-0.038821\t0.675410\t1.405862\t2.112551\tsurprise\n",
    "1\t0.238770\t0.361816\t0.478516\t0.473633\t0.485352\t0.476074\t0.472656\t0.468262\t0.472656\t0.486816\t...\t2.192987\t1.378691\t1.542983\t1.766679\t-1.586487\t-0.501919\t-3.159530\t-5.015890\t-0.942531\tsurprise\n",
    "2\t0.299805\t0.419922\t0.525879\t0.459473\t0.378418\t0.326172\t0.282227\t0.260254\t0.258789\t0.253906\t...\t0.518931\t0.839123\t1.308965\t1.855116\t2.404975\t2.896071\t3.282977\t3.541091\t3.666846\tsurprise\n",
    "3\t0.252930\t0.382812\t0.497559\t0.497070\t0.487793\t0.472656\t0.482422\t0.484375\t0.497070\t0.510254\t...\t-2.445378\t-2.712779\t-1.120462\t-3.266942\t7.691891\t7.443986\t-2.031003\t-2.095720\t-1.418903\tsurprise\n",
    "4\t0.400879\t0.591309\t0.783203\t0.777832\t0.771973\t0.777832\t0.771973\t0.775391\t0.758789\t0.580078\t...\t2.585884\t2.746578\t2.223155\t0.986784\t-0.730886\t-2.531058\t-4.002848\t-4.849192\t-4.929412\tneutral\n",
    "5 rows × 2377 columns\n",
    "\n",
    "print(Emotions.isna().any())\n",
    "0           False\n",
    "1           False\n",
    "2           False\n",
    "3           False\n",
    "4           False\n",
    "            ...  \n",
    "2372         True\n",
    "2373         True\n",
    "2374         True\n",
    "2375         True\n",
    "Emotions    False\n",
    "Length: 2377, dtype: bool\n",
    "Emotions=Emotions.fillna(0)\n",
    "print(Emotions.isna().any())\n",
    "Emotions.shape\n",
    "0           False\n",
    "1           False\n",
    "2           False\n",
    "3           False\n",
    "4           False\n",
    "            ...  \n",
    "2372        False\n",
    "2373        False\n",
    "2374        False\n",
    "2375        False\n",
    "Emotions    False\n",
    "Length: 2377, dtype: bool\n",
    "(48648, 2377)\n",
    "np.sum(Emotions.isna())\n",
    "0           0\n",
    "1           0\n",
    "2           0\n",
    "3           0\n",
    "4           0\n",
    "           ..\n",
    "2372        0\n",
    "2373        0\n",
    "2374        0\n",
    "2375        0\n",
    "Emotions    0\n",
    "Length: 2377, dtype: int64\n",
    "Data preparation\n",
    "#taking all rows and all cols without last col for X which include features\n",
    "#taking last col for Y, which include the emotions\n",
    "\n",
    "\n",
    "X = Emotions.iloc[: ,:-1].values\n",
    "Y = Emotions['Emotions'].values\n",
    "# As this is a multiclass classification problem onehotencoding our Y\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n",
    "print(Y.shape)\n",
    "X.shape\n",
    "(48648, 7)\n",
    "(48648, 2376)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42,test_size=0.2, shuffle=True)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "((38918, 2376), (38918, 7), (9730, 2376), (9730, 7))\n",
    "#reshape for lstm\n",
    "X_train = x_train.reshape(x_train.shape[0] , x_train.shape[1] , 1)\n",
    "X_test = x_test.reshape(x_test.shape[0] , x_test.shape[1] , 1)\n",
    "# scaling our data with sklearn's Standard scaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "((38918, 2376), (38918, 7), (9730, 2376), (9730, 7))\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM,BatchNormalization , GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "Applying early stopping for all models\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n",
    "model_checkpoint = ModelCheckpoint('best_model1_weights.h5', monitor='val_accuracy', save_best_only=True)\n",
    "early_stop=EarlyStopping(monitor='val_acc',mode='auto',patience=5,restore_best_weights=True)\n",
    "lr_reduction=ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.5,min_lr=0.00001)\n",
    "LSTM Model\n",
    "Model that have lstm layers take alot of time if you have much free time enjoy with it\n",
    "\n",
    "\"\"\"model01=Sequential()\n",
    "model01.add(LSTM(128,return_sequences=True,input_shape=(x_train.shape[1],1)))\n",
    "model01.add(Dropout(0.2))\n",
    "model01.add(LSTM(128,return_sequences=True))\n",
    "#model01.add(Dropout(0.2))\n",
    "model01.add(LSTM(128,return_sequences=True))\n",
    "#model01.add(Dropout(0.2))\n",
    "model01.add(LSTM(128,return_sequences=True))\n",
    "#model01.add(Dropout(0.2))\n",
    "model01.add(LSTM(128,return_sequences=True))\n",
    "#model01.add(Dropout(0.2))\n",
    "model01.add(LSTM(128,return_sequences=True))\n",
    "#model01.add(Dropout(0.3))\n",
    "model01.add(LSTM(128))\n",
    "#model01.add(Dropout(0.3))\n",
    "model01.add(Dense(7,activation = 'softmax'))\n",
    "model01.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model01.summary()\"\"\"\n",
    "\"model01=Sequential()\\nmodel01.add(LSTM(128,return_sequences=True,input_shape=(x_train.shape[1],1)))\\nmodel01.add(Dropout(0.2))\\nmodel01.add(LSTM(128,return_sequences=True))\\n#model01.add(Dropout(0.2))\\nmodel01.add(LSTM(128,return_sequences=True))\\n#model01.add(Dropout(0.2))\\nmodel01.add(LSTM(128,return_sequences=True))\\n#model01.add(Dropout(0.2))\\nmodel01.add(LSTM(128,return_sequences=True))\\n#model01.add(Dropout(0.2))\\nmodel01.add(LSTM(128,return_sequences=True))\\n#model01.add(Dropout(0.3))\\nmodel01.add(LSTM(128))\\n#model01.add(Dropout(0.3))\\nmodel01.add(Dense(7,activation = 'softmax'))\\nmodel01.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\\nmodel01.summary()\"\n",
    "\"\"\"hist=model01.fit(X_train, y_train,\n",
    "            epochs=20,\n",
    "            validation_data=(X_test, y_test),batch_size=64,\n",
    "            verbose=1)\"\"\"\n",
    "'hist=model01.fit(X_train, y_train,\\n            epochs=20,\\n            validation_data=(X_test, y_test),batch_size=64,\\n            verbose=1)'\n",
    "\"\"\"print(\"Accuracy of our model on test data : \" , model01.evaluate(X_test,y_test)[1]*100 , \"%\")\n",
    "epochs = [i for i in range(20)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = hist.history['accuracy']\n",
    "train_loss = hist.history['loss']\n",
    "test_acc = hist.history['val_accuracy']\n",
    "test_loss = hist.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()\"\"\"\n",
    "'print(\"Accuracy of our model on test data : \" , model01.evaluate(X_test,y_test)[1]*100 , \"%\")\\nepochs = [i for i in range(20)]\\nfig , ax = plt.subplots(1,2)\\ntrain_acc = hist.history[\\'accuracy\\']\\ntrain_loss = hist.history[\\'loss\\']\\ntest_acc = hist.history[\\'val_accuracy\\']\\ntest_loss = hist.history[\\'val_loss\\']\\n\\nfig.set_size_inches(20,6)\\nax[0].plot(epochs , train_loss , label = \\'Training Loss\\')\\nax[0].plot(epochs , test_loss , label = \\'Testing Loss\\')\\nax[0].set_title(\\'Training & Testing Loss\\')\\nax[0].legend()\\nax[0].set_xlabel(\"Epochs\")\\n\\nax[1].plot(epochs , train_acc , label = \\'Training Accuracy\\')\\nax[1].plot(epochs , test_acc , label = \\'Testing Accuracy\\')\\nax[1].set_title(\\'Training & Testing Accuracy\\')\\nax[1].legend()\\nax[1].set_xlabel(\"Epochs\")\\nplt.show()'\n",
    "CNN model\n",
    "#Reshape for CNN_LSTM MODEL\n",
    "\n",
    "x_traincnn =np.expand_dims(x_train, axis=2)\n",
    "x_testcnn= np.expand_dims(x_test, axis=2)\n",
    "x_traincnn.shape, y_train.shape, x_testcnn.shape, y_test.shape\n",
    "#x_testcnn[0]\n",
    "((38918, 2376, 1), (38918, 7), (9730, 2376, 1), (9730, 7))\n",
    "import tensorflow.keras.layers as L\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    L.Conv1D(512,kernel_size=5, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1],1)),\n",
    "    L.BatchNormalization(),\n",
    "    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n",
    "    \n",
    "    L.Conv1D(512,kernel_size=5,strides=1,padding='same',activation='relu'),\n",
    "    L.BatchNormalization(),\n",
    "    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n",
    "    Dropout(0.2),  # Add dropout layer after the second max pooling layer\n",
    "    \n",
    "    L.Conv1D(256,kernel_size=5,strides=1,padding='same',activation='relu'),\n",
    "    L.BatchNormalization(),\n",
    "    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n",
    "    \n",
    "    L.Conv1D(256,kernel_size=3,strides=1,padding='same',activation='relu'),\n",
    "    L.BatchNormalization(),\n",
    "    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n",
    "    Dropout(0.2),  # Add dropout layer after the fourth max pooling layer\n",
    "    \n",
    "    L.Conv1D(128,kernel_size=3,strides=1,padding='same',activation='relu'),\n",
    "    L.BatchNormalization(),\n",
    "    L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n",
    "    Dropout(0.2),  # Add dropout layer after the fifth max pooling layer\n",
    "    \n",
    "    L.Flatten(),\n",
    "    L.Dense(512,activation='relu'),\n",
    "    L.BatchNormalization(),\n",
    "    L.Dense(7,activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\n",
    "model.summary()\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d (Conv1D)              (None, 2376, 512)         3072      \n",
    "_________________________________________________________________\n",
    "batch_normalization (BatchNo (None, 2376, 512)         2048      \n",
    "_________________________________________________________________\n",
    "max_pooling1d (MaxPooling1D) (None, 1188, 512)         0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 1188, 512)         1311232   \n",
    "_________________________________________________________________\n",
    "batch_normalization_1 (Batch (None, 1188, 512)         2048      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 594, 512)          0         \n",
    "_________________________________________________________________\n",
    "dropout (Dropout)            (None, 594, 512)          0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 594, 256)          655616    \n",
    "_________________________________________________________________\n",
    "batch_normalization_2 (Batch (None, 594, 256)          1024      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_2 (MaxPooling1 (None, 297, 256)          0         \n",
    "_________________________________________________________________\n",
    "conv1d_3 (Conv1D)            (None, 297, 256)          196864    \n",
    "_________________________________________________________________\n",
    "batch_normalization_3 (Batch (None, 297, 256)          1024      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_3 (MaxPooling1 (None, 149, 256)          0         \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 149, 256)          0         \n",
    "_________________________________________________________________\n",
    "conv1d_4 (Conv1D)            (None, 149, 128)          98432     \n",
    "_________________________________________________________________\n",
    "batch_normalization_4 (Batch (None, 149, 128)          512       \n",
    "_________________________________________________________________\n",
    "max_pooling1d_4 (MaxPooling1 (None, 75, 128)           0         \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 75, 128)           0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 9600)              0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 512)               4915712   \n",
    "_________________________________________________________________\n",
    "batch_normalization_5 (Batch (None, 512)               2048      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 7)                 3591      \n",
    "=================================================================\n",
    "Total params: 7,193,223\n",
    "Trainable params: 7,188,871\n",
    "Non-trainable params: 4,352\n",
    "_________________________________________________________________\n",
    "history=model.fit(x_traincnn, y_train, epochs=50, validation_data=(x_testcnn, y_test), batch_size=64,callbacks=[early_stop,lr_reduction,model_checkpoint])\n",
    "Epoch 1/50\n",
    "609/609 [==============================] - 116s 176ms/step - loss: 1.4134 - accuracy: 0.4612 - val_loss: 1.5701 - val_accuracy: 0.4272\n",
    "Epoch 2/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 1.1311 - accuracy: 0.5590 - val_loss: 1.1125 - val_accuracy: 0.5736\n",
    "Epoch 3/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.9911 - accuracy: 0.6172 - val_loss: 1.2060 - val_accuracy: 0.5508\n",
    "Epoch 4/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.8980 - accuracy: 0.6541 - val_loss: 1.1263 - val_accuracy: 0.5845\n",
    "Epoch 5/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.7886 - accuracy: 0.6979 - val_loss: 0.8593 - val_accuracy: 0.6742\n",
    "Epoch 6/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.6691 - accuracy: 0.7490 - val_loss: 0.9310 - val_accuracy: 0.6601\n",
    "Epoch 7/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.5480 - accuracy: 0.7971 - val_loss: 0.6999 - val_accuracy: 0.7398\n",
    "Epoch 8/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.4135 - accuracy: 0.8510 - val_loss: 0.5057 - val_accuracy: 0.8207\n",
    "Epoch 9/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.3312 - accuracy: 0.8807 - val_loss: 0.4205 - val_accuracy: 0.8527\n",
    "Epoch 10/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.2301 - accuracy: 0.9197 - val_loss: 0.4459 - val_accuracy: 0.8460\n",
    "Epoch 11/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.2040 - accuracy: 0.9271 - val_loss: 0.4964 - val_accuracy: 0.8363\n",
    "Epoch 12/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.1874 - accuracy: 0.9354 - val_loss: 0.3515 - val_accuracy: 0.8840\n",
    "Epoch 13/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.1076 - accuracy: 0.9645 - val_loss: 0.2396 - val_accuracy: 0.9207\n",
    "Epoch 14/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0928 - accuracy: 0.9696 - val_loss: 0.2972 - val_accuracy: 0.9066\n",
    "Epoch 15/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.1134 - accuracy: 0.9603 - val_loss: 0.2724 - val_accuracy: 0.9150\n",
    "Epoch 16/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.1116 - accuracy: 0.9613 - val_loss: 0.2140 - val_accuracy: 0.9322\n",
    "Epoch 17/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0688 - accuracy: 0.9772 - val_loss: 0.1873 - val_accuracy: 0.9425\n",
    "Epoch 18/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0640 - accuracy: 0.9792 - val_loss: 0.1703 - val_accuracy: 0.9465\n",
    "Epoch 19/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0564 - accuracy: 0.9821 - val_loss: 0.2515 - val_accuracy: 0.9250\n",
    "Epoch 20/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.1101 - accuracy: 0.9619 - val_loss: 0.1996 - val_accuracy: 0.9404\n",
    "Epoch 21/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0452 - accuracy: 0.9853 - val_loss: 0.1720 - val_accuracy: 0.9509\n",
    "Epoch 22/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0513 - accuracy: 0.9829 - val_loss: 0.1676 - val_accuracy: 0.9505\n",
    "Epoch 23/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.1122 - accuracy: 0.9637 - val_loss: 0.1462 - val_accuracy: 0.9590\n",
    "Epoch 24/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0334 - accuracy: 0.9896 - val_loss: 0.1344 - val_accuracy: 0.9608\n",
    "Epoch 25/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0389 - accuracy: 0.9878 - val_loss: 0.1503 - val_accuracy: 0.9546\n",
    "Epoch 26/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0362 - accuracy: 0.9879 - val_loss: 0.1732 - val_accuracy: 0.9488\n",
    "Epoch 27/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0478 - accuracy: 0.9842 - val_loss: 0.2462 - val_accuracy: 0.9292\n",
    "Epoch 28/50\n",
    "609/609 [==============================] - 110s 181ms/step - loss: 0.0388 - accuracy: 0.9872 - val_loss: 0.1724 - val_accuracy: 0.9516\n",
    "Epoch 29/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0837 - accuracy: 0.9722 - val_loss: 0.1379 - val_accuracy: 0.9621\n",
    "Epoch 30/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0291 - accuracy: 0.9903 - val_loss: 0.1596 - val_accuracy: 0.9545\n",
    "Epoch 31/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0290 - accuracy: 0.9911 - val_loss: 0.1833 - val_accuracy: 0.9490\n",
    "Epoch 32/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0411 - accuracy: 0.9862 - val_loss: 0.1720 - val_accuracy: 0.9523\n",
    "Epoch 33/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0442 - accuracy: 0.9855 - val_loss: 0.1574 - val_accuracy: 0.9568\n",
    "Epoch 34/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0419 - accuracy: 0.9862 - val_loss: 0.1582 - val_accuracy: 0.9592\n",
    "Epoch 35/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0407 - accuracy: 0.9866 - val_loss: 0.1724 - val_accuracy: 0.9572\n",
    "Epoch 36/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0336 - accuracy: 0.9893 - val_loss: 0.1515 - val_accuracy: 0.9598\n",
    "Epoch 37/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0293 - accuracy: 0.9904 - val_loss: 0.1643 - val_accuracy: 0.9576\n",
    "Epoch 38/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0611 - accuracy: 0.9804 - val_loss: 0.1490 - val_accuracy: 0.9612\n",
    "Epoch 39/50\n",
    "609/609 [==============================] - 110s 181ms/step - loss: 0.0253 - accuracy: 0.9920 - val_loss: 0.1357 - val_accuracy: 0.9637\n",
    "Epoch 40/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0245 - accuracy: 0.9922 - val_loss: 0.1783 - val_accuracy: 0.9520\n",
    "Epoch 41/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.1003 - accuracy: 0.9692 - val_loss: 0.1865 - val_accuracy: 0.9479\n",
    "Epoch 42/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.2109 - accuracy: 0.9397 - val_loss: 0.1460 - val_accuracy: 0.9579\n",
    "Epoch 43/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0615 - accuracy: 0.9788 - val_loss: 0.1220 - val_accuracy: 0.9656\n",
    "Epoch 44/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0219 - accuracy: 0.9931 - val_loss: 0.1084 - val_accuracy: 0.9692\n",
    "Epoch 45/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.1063 - val_accuracy: 0.9715\n",
    "Epoch 46/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0165 - accuracy: 0.9951 - val_loss: 0.1047 - val_accuracy: 0.9704\n",
    "Epoch 47/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.1161 - val_accuracy: 0.9724\n",
    "Epoch 48/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0149 - accuracy: 0.9957 - val_loss: 0.1117 - val_accuracy: 0.9725\n",
    "Epoch 49/50\n",
    "609/609 [==============================] - 106s 174ms/step - loss: 0.0293 - accuracy: 0.9908 - val_loss: 0.1536 - val_accuracy: 0.9616\n",
    "Epoch 50/50\n",
    "609/609 [==============================] - 106s 175ms/step - loss: 0.1081 - accuracy: 0.9673 - val_loss: 0.1318 - val_accuracy: 0.9634\n",
    "print(\"Accuracy of our model on test data : \" , model.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n",
    "\n",
    "epochs = [i for i in range(50)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "test_acc = history.history['val_accuracy']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()\n",
    "305/305 [==============================] - 7s 24ms/step - loss: 0.1318 - accuracy: 0.9634\n",
    "Accuracy of our model on test data :  96.34121060371399 %\n",
    "\n",
    "# predicting on test data.\n",
    "pred_test0 = model.predict(x_testcnn)\n",
    "y_pred0 = encoder.inverse_transform(pred_test0)\n",
    "y_test0 = encoder.inverse_transform(y_test)\n",
    "\n",
    "# Check for random predictions\n",
    "df0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
    "df0['Predicted Labels'] = y_pred0.flatten()\n",
    "df0['Actual Labels'] = y_test0.flatten()\n",
    "\n",
    "df0.head(10)\n",
    "Predicted Labels\tActual Labels\n",
    "0\tangry\tangry\n",
    "1\tangry\tangry\n",
    "2\tdisgust\tdisgust\n",
    "3\thappy\thappy\n",
    "4\tfear\tfear\n",
    "5\thappy\thappy\n",
    "6\thappy\thappy\n",
    "7\tfear\tfear\n",
    "8\tfear\tfear\n",
    "9\tsurprise\tsurprise\n",
    "df0\n",
    "Predicted Labels\tActual Labels\n",
    "0\tangry\tangry\n",
    "1\tangry\tangry\n",
    "2\tdisgust\tdisgust\n",
    "3\thappy\thappy\n",
    "4\tfear\tfear\n",
    "...\t...\t...\n",
    "9725\tfear\tfear\n",
    "9726\tdisgust\tdisgust\n",
    "9727\tneutral\tneutral\n",
    "9728\tsad\tsad\n",
    "9729\tfear\tfear\n",
    "9730 rows × 2 columns\n",
    "\n",
    "Some plots of multi_model\n",
    "\n",
    "CLSTM Model\n",
    "Model that have lstm layers take alot of time if you have much free time enjoy with it\n",
    "\n",
    "Another model (CLSTM) omnia model\n",
    "\n",
    "#Build the model\n",
    "\n",
    "# define model\n",
    "\"\"\"model000 = Sequential()\n",
    "model000.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(X.shape[1], 1)))\n",
    "model000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
    "model000.add(BatchNormalization())\n",
    "model000.add(Dropout(0.3))\n",
    "\n",
    "          \n",
    "model000.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
    "model000.add(BatchNormalization())\n",
    "model000.add(Dropout(0.3))\n",
    "\n",
    "model000.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
    "model000.add(BatchNormalization())\n",
    "model000.add(Dropout(0.3))\n",
    "          \n",
    "model000.add(LSTM(128, return_sequences=True)) \n",
    "model000.add(Dropout(0.3))\n",
    "\n",
    "model000.add(LSTM(128, return_sequences=True)) \n",
    "model000.add(Dropout(0.3))\n",
    "model000.add(LSTM(128))\n",
    "model000.add(Dropout(0.3))\n",
    "\n",
    "model000.add(Dense(128, activation='relu'))\n",
    "#model000.add(Dropout(0.3))\n",
    "\n",
    "model000.add(Dense(64, activation='relu'))\n",
    "#model000.add(Dropout(0.3))\n",
    "\n",
    "model000.add(Dense(32, activation='relu'))\n",
    "#model000.add(Dropout(0.3))\n",
    "\n",
    "model000.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model000.summary()\"\"\"\n",
    "\"model000 = Sequential()\\nmodel000.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(X.shape[1], 1)))\\nmodel000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\\nmodel000.add(BatchNormalization())\\nmodel000.add(Dropout(0.3))\\n\\n          \\nmodel000.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\\nmodel000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\\nmodel000.add(BatchNormalization())\\nmodel000.add(Dropout(0.3))\\n\\nmodel000.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\\nmodel000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\\nmodel000.add(BatchNormalization())\\nmodel000.add(Dropout(0.3))\\n          \\nmodel000.add(LSTM(128, return_sequences=True)) \\nmodel000.add(Dropout(0.3))\\n\\nmodel000.add(LSTM(128, return_sequences=True)) \\nmodel000.add(Dropout(0.3))\\nmodel000.add(LSTM(128))\\nmodel000.add(Dropout(0.3))\\n\\nmodel000.add(Dense(128, activation='relu'))\\n#model000.add(Dropout(0.3))\\n\\nmodel000.add(Dense(64, activation='relu'))\\n#model000.add(Dropout(0.3))\\n\\nmodel000.add(Dense(32, activation='relu'))\\n#model000.add(Dropout(0.3))\\n\\nmodel000.add(Dense(7, activation='softmax'))\\n\\n\\n\\nmodel000.summary()\"\n",
    "\"\"\"from keras.utils.vis_utils import plot_model\n",
    "plot_model( model000, show_shapes=True, show_layer_names=True, to_file='model000.png')\"\"\"\n",
    "\"from keras.utils.vis_utils import plot_model\\nplot_model( model000, show_shapes=True, show_layer_names=True, to_file='model000.png')\"\n",
    "\"\"\"model000.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"\n",
    "\"model000.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\"\n",
    "\"\"\"hist1=model000.fit(x_traincnn, y_train, batch_size=64, epochs=40, validation_data=(x_testcnn, y_test))\"\"\"\n",
    "'hist1=model000.fit(x_traincnn, y_train, batch_size=64, epochs=40, validation_data=(x_testcnn, y_test))'\n",
    "\"\"\"print(\"Accuracy of our model on test data : \" , model000.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n",
    "epochs = [i for i in range(40)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = hist1.history['accuracy']\n",
    "train_loss = hist1.history['loss']\n",
    "test_acc = hist1.history['val_accuracy']\n",
    "test_loss = hist1.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()\"\"\"\n",
    "'print(\"Accuracy of our model on test data : \" , model000.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\\nepochs = [i for i in range(40)]\\nfig , ax = plt.subplots(1,2)\\ntrain_acc = hist1.history[\\'accuracy\\']\\ntrain_loss = hist1.history[\\'loss\\']\\ntest_acc = hist1.history[\\'val_accuracy\\']\\ntest_loss = hist1.history[\\'val_loss\\']\\n\\nfig.set_size_inches(20,6)\\nax[0].plot(epochs , train_loss , label = \\'Training Loss\\')\\nax[0].plot(epochs , test_loss , label = \\'Testing Loss\\')\\nax[0].set_title(\\'Training & Testing Loss\\')\\nax[0].legend()\\nax[0].set_xlabel(\"Epochs\")\\n\\nax[1].plot(epochs , train_acc , label = \\'Training Accuracy\\')\\nax[1].plot(epochs , test_acc , label = \\'Testing Accuracy\\')\\nax[1].set_title(\\'Training & Testing Accuracy\\')\\nax[1].legend()\\nax[1].set_xlabel(\"Epochs\")\\nplt.show()'\n",
    "# predicting on test data.\n",
    "\"\"\"pred_test00 = model000.predict(x_testcnn)\n",
    "y_pred00 = encoder.inverse_transform(pred_test)\n",
    "y_test00 = encoder.inverse_transform(y_test)\n",
    "\n",
    "# Check for random predictions\n",
    "df0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
    "df0['Predicted Labels'] = y_pred00.flatten()\n",
    "df0['Actual Labels'] = y_test00.flatten()\n",
    "\n",
    "df0.head(10)\"\"\"\n",
    "\"pred_test00 = model000.predict(x_testcnn)\\ny_pred00 = encoder.inverse_transform(pred_test)\\ny_test00 = encoder.inverse_transform(y_test)\\n\\n# Check for random predictions\\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\\ndf0['Predicted Labels'] = y_pred00.flatten()\\ndf0['Actual Labels'] = y_test00.flatten()\\n\\ndf0.head(10)\"\n",
    "Evalutation\n",
    "Results of best model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cm = confusion_matrix(y_test0, y_pred0)\n",
    "plt.figure(figsize = (12, 10))\n",
    "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
    "#cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='.2f')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()\n",
    "print(classification_report(y_test0, y_pred0))\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       angry       0.96      0.97      0.97      1484\n",
    "     disgust       0.97      0.95      0.96      1558\n",
    "        fear       0.96      0.97      0.96      1505\n",
    "       happy       0.96      0.95      0.96      1619\n",
    "     neutral       0.97      0.98      0.97      1558\n",
    "         sad       0.96      0.97      0.96      1478\n",
    "    surprise       0.98      0.97      0.97       528\n",
    "\n",
    "    accuracy                           0.96      9730\n",
    "   macro avg       0.96      0.96      0.96      9730\n",
    "weighted avg       0.96      0.96      0.96      9730\n",
    "\n",
    "Saving Best Model\n",
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"CNN_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"CNN_model_weights.h5\")\n",
    "print(\"Saved model to disk\") \n",
    "Saved model to disk\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "json_file = open('/kaggle/working/CNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"/kaggle/working/best_model1_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "Loaded model from disk\n",
    "loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_testcnn,y_test)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "305/305 [==============================] - 8s 24ms/step - loss: 0.1117 - accuracy: 0.9725\n",
    "accuracy: 97.25%\n",
    "Saving and Loading our Stnadrad Scaler and encoder\n",
    "To save the StandardScaler object to use it later in a Flask API\n",
    "pickle file\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Saving scaler\n",
    "with open('scaler2.pickle', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Loading scaler\n",
    "with open('scaler2.pickle', 'rb') as f:\n",
    "    scaler2 = pickle.load(f)\n",
    "\n",
    "# Saving encoder\n",
    "with open('encoder2.pickle', 'wb') as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "# Loading encoder\n",
    "with open('encoder2.pickle', 'rb') as f:\n",
    "    encoder2 = pickle.load(f)\n",
    "\n",
    "    \n",
    "print(\"Done\")    \n",
    "Done\n",
    "Test script\n",
    "That can predict new record\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "json_file = open('/kaggle/working/CNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"/kaggle/working/best_model1_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "Loaded model from disk\n",
    "import pickle\n",
    "\n",
    "with open('/kaggle/working/scaler2.pickle', 'rb') as f:\n",
    "    scaler2 = pickle.load(f)\n",
    "    \n",
    "with open('/kaggle/working/encoder2.pickle', 'rb') as f:\n",
    "    encoder2 = pickle.load(f)\n",
    "\n",
    "    \n",
    "print(\"Done\")    \n",
    "Done\n",
    "import librosa\n",
    "def zcr(data,frame_length,hop_length):\n",
    "    zcr=librosa.feature.zero_crossing_rate(data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "def rmse(data,frame_length=2048,hop_length=512):\n",
    "    rmse=librosa.feature.rms(data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(rmse)\n",
    "def mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n",
    "    mfcc=librosa.feature.mfcc(data,sr=sr)\n",
    "    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n",
    "\n",
    "def extract_features(data,sr=22050,frame_length=2048,hop_length=512):\n",
    "    result=np.array([])\n",
    "    \n",
    "    result=np.hstack((result,\n",
    "                      zcr(data,frame_length,hop_length),\n",
    "                      rmse(data,frame_length,hop_length),\n",
    "                      mfcc(data,sr,frame_length,hop_length)\n",
    "                     ))\n",
    "    return result\n",
    "def get_predict_feat(path):\n",
    "    d, s_rate= librosa.load(path, duration=2.5, offset=0.6)\n",
    "    res=extract_features(d)\n",
    "    result=np.array(res)\n",
    "    result=np.reshape(result,newshape=(1,2376))\n",
    "    i_result = scaler2.transform(result)\n",
    "    final_result=np.expand_dims(i_result, axis=2)\n",
    "    \n",
    "    return final_result\n",
    "res=get_predict_feat(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-07-01-01-01-01.wav\")\n",
    "print(res.shape)\n",
    "(1, 2376, 1)\n",
    "emotions1={1:'Neutral', 2:'Calm', 3:'Happy', 4:'Sad', 5:'Angry', 6:'Fear', 7:'Disgust',8:'Surprise'}\n",
    "def prediction(path1):\n",
    "    res=get_predict_feat(path1)\n",
    "    predictions=loaded_model.predict(res)\n",
    "    y_pred = encoder2.inverse_transform(predictions)\n",
    "    print(y_pred[0][0])    \n",
    "prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_02/03-01-01-01-01-01-02.wav\")\n",
    "neutral\n",
    "prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav\")\n",
    "neutral\n",
    "prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-05-01-02-02-01.wav\")\n",
    "angry\n",
    "prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_21/03-01-04-02-02-02-21.wav\")\n",
    "sad\n",
    "prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_02/03-01-06-01-02-02-02.wav\")\n",
    "fear\n",
    "prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-08-01-01-01-01.wav\")\n",
    "surprise\n",
    "prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-07-01-01-01-01.wav\")\n",
    "disgust"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
